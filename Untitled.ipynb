{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-32d8dcb1ba15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# train gene feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_variation_feature_responseCoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_gv_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Variation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# test gene feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtest_variation_feature_responseCoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_gv_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Variation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
    "#------------------------------\n",
    "\n",
    "\n",
    "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
    "# ----------------------------\n",
    "# default paramters\n",
    "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n",
    "#\n",
    "# some of the methods of CalibratedClassifierCV()\n",
    "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict the target of new samples.\n",
    "# predict_proba(X)\tPosterior probabilities of classification\n",
    "#-------------------------------------\n",
    "# video link:\n",
    "#-------------------------------------\n",
    "\n",
    "alpha = [10 ** x for x in range(-6, 3)]\n",
    "cv_log_error_array = []\n",
    "for i in alpha:\n",
    "    print(\"for alpha =\", i)\n",
    "    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_x_onehotCoding_LR, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding_LR, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding_LR)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_x_onehotCoding_LR, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding_LR, train_y)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_onehotCoding_LR)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_onehotCoding_LR)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_onehotCoding_LR)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
    "#------------------------------\n",
    "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "predict_and_plot_confusion_matrix(train_x_onehotCoding_LR, train_y, cv_x_onehotCoding_LR, cv_y, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read more about support vector machines with linear kernals here http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "# --------------------------------\n",
    "# default parameters \n",
    "# SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False, tol=0.001, \n",
    "# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n",
    "\n",
    "# Some of methods of SVM()\n",
    "# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n",
    "# predict(X)\tPerform classification on samples in X.\n",
    "# --------------------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/mathematical-derivation-copy-8/\n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
    "# ----------------------------\n",
    "# default paramters\n",
    "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n",
    "#\n",
    "# some of the methods of CalibratedClassifierCV()\n",
    "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict the target of new samples.\n",
    "# predict_proba(X)\tPosterior probabilities of classification\n",
    "#-------------------------------------\n",
    "# video link:\n",
    "#-------------------------------------\n",
    "\n",
    "alpha = [10 ** x for x in range(-5, 3)]\n",
    "cv_log_error_array = []\n",
    "for i in alpha:\n",
    "    print(\"for C =\", i)\n",
    "#     clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n",
    "    clf = SGDClassifier( class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42)\n",
    "    clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "# clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n",
    "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n",
    "clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read more about support vector machines with linear kernals here http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "# --------------------------------\n",
    "# default parameters \n",
    "# SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False, tol=0.001, \n",
    "# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=’ovr’, random_state=None)\n",
    "\n",
    "# Some of methods of SVM()\n",
    "# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n",
    "# predict(X)\tPerform classification on samples in X.\n",
    "# --------------------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/mathematical-derivation-copy-8/\n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "# clf = SVC(C=alpha[best_alpha],kernel='linear',probability=True, class_weight='balanced')\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\n",
    "predict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
    "#------------------------------\n",
    "\n",
    "\n",
    "# find more about CalibratedClassifierCV here at http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
    "# ----------------------------\n",
    "# default paramters\n",
    "# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=’sigmoid’, cv=3)\n",
    "#\n",
    "# some of the methods of CalibratedClassifierCV()\n",
    "# fit(X, y[, sample_weight])\tFit the calibrated model\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict the target of new samples.\n",
    "# predict_proba(X)\tPosterior probabilities of classification\n",
    "#-------------------------------------\n",
    "# video link:\n",
    "#-------------------------------------\n",
    "\n",
    "alpha = [10 ** x for x in range(-6, 3)]\n",
    "cv_log_error_array = []\n",
    "for i in alpha:\n",
    "    print(\"for alpha =\", i)\n",
    "    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_x_onehotCoding_LR, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding_LR, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding_LR)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, cv_log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
    "    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_x_onehotCoding_LR, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding_LR, train_y)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_onehotCoding_LR)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_onehotCoding_LR)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_onehotCoding_LR)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1/\n",
    "#------------------------------\n",
    "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "predict_and_plot_confusion_matrix(train_x_onehotCoding_LR, train_y, cv_x_onehotCoding_LR, cv_y, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h5>4.3.1.3.1. Correctly Classified point</h5>\n",
    "# from tabulate import tabulate\n",
    "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_x_onehotCoding,train_y)\n",
    "test_point_index = 1\n",
    "no_feature = 100\n",
    "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
    "print(\"Predicted Class :\", predicted_cls[0])\n",
    "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
    "print(\"Actual Class :\", test_y[test_point_index])\n",
    "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
    "print(\"-\"*50)\n",
    "get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h5>4.3.1.3.2. Incorrectly Classified point</h5>\n",
    "test_point_index = 100\n",
    "no_feature = 500\n",
    "predicted_cls = sig_clf.predict(test_x_onehotCoding_LR[test_point_index])\n",
    "print(\"Predicted Class :\", predicted_cls[0])\n",
    "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding_LR[test_point_index]),4))\n",
    "print(\"Actual Class :\", test_y[test_point_index])\n",
    "indices = np.argsort(-clf.coef_)[predicted_cls-1][:,:no_feature]\n",
    "print(\"-\"*50)\n",
    "get_impfeature_namesLR(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table, th, td {\n",
    "    border: 1px solid black;\n",
    "    border-collapse: collapse;\n",
    "}\n",
    "th, td {\n",
    "    padding: 25px;\n",
    "    text-align: left;\n",
    "}\n",
    "\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h2>Summary</h2>\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Model</th>\n",
    "    <th>Best Hyperparameter</th> \n",
    "    <th>Train error</th>\n",
    "    <th>Cv error</th>\n",
    "    <th>Test error</th>\n",
    "    <th>Misclassified Points(percentage)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Naive Bayes + one-hot Encoding</td>\n",
    "    <td>0.001</td>\n",
    "    <td>0.60</td>\n",
    "    <td>1.17</td>\n",
    "    <td>1.23</td>\n",
    "    <td>36.84%</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>K-NN + Response coding</td>\n",
    "    <td>99</td>\n",
    "    <td>0.94</td>\n",
    "    <td>1.08</td>\n",
    "    <td>1.11</td>\n",
    "    <td>38.15%</td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <td>Logistic Regression + one-hot encoding + count vectorizer(unigram+bigram) + class balance</td>\n",
    "    <td>0.1</td>\n",
    "    <td>0.68</td>\n",
    "    <td>1.24</td>\n",
    "    <td>1.24</td>\n",
    "    <td>40.60%</td>\n",
    "  </tr> \n",
    "    <tr>\n",
    "    <td>Logistic Regression + one-hot encoding + count vectorizer(unigram+bigram) + without class balance</td>\n",
    "    <td>0.1</td>\n",
    "    <td>0.67</td>\n",
    "    <td>1.23</td>\n",
    "    <td>1.15</td>\n",
    "    <td>40.41</td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <td>Logistic Regression + one-hot encoding + tfidf vectorizer(with 2000 max words) + Feature Engineering(square root) </td>\n",
    "    <td>0.001</td>\n",
    "    <td><b>0.62</b></td>\n",
    "     <td><b>0.96</b></td>\n",
    "     <td><b>0.97</b></td>\n",
    "     <td><b>31.27%</b></td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <td>Linear SVM + one-hot encoding</td>\n",
    "    <td>0.001</td>\n",
    "    <td>0.60</td>\n",
    "    <td>1.05</td>\n",
    "    <td>1.04</td>\n",
    "    <td>33.83%</td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <td>Random Forest + one-hot encoding</td>\n",
    "    <td>2000</td>\n",
    "    <td>0.86</td>\n",
    "    <td>1.15</td>\n",
    "    <td>1.18</td>\n",
    "    <td>42.10%</td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <td>Random Forest + Response Coding</td>\n",
    "    <td>100</td>\n",
    "    <td>0.06</td>\n",
    "    <td>1.38</td>\n",
    "    <td>1.37</td>\n",
    "    <td>50.37%</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Stack Classifier(LR+LrSVM+NB) + Meta classifier(LR) + one-hot encoding</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.59</td>\n",
    "    <td>1.11</td>\n",
    "    <td>1.16</td>\n",
    "    <td>36.99%</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Maximum voting Classifier(LR+LrSVM+RF) + one-hot encoding</td>\n",
    "    <td>0.10</td>\n",
    "    <td>0.87</td>\n",
    "    <td>1.16</td>\n",
    "    <td>1.20</td>\n",
    "    <td>38.34%</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
